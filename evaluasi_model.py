# -*- coding: utf-8 -*-
"""evaluasi model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16PovS9_3-j0GdXHQc9rTLTipELpRL0fm
"""

# Import necessary libraries
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score # Import precision_score, recall_score, f1_score
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn import metrics
from sklearn.feature_extraction.text import TfidfVectorizer # Import TfidfVectorizer
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

"""load data"""

df = pd.read_csv("/content/Hasil prepocessing (5).csv")
df.head()

"""split data"""

# Split Data
X = df['stemming']
y = df['label']

# Tahap 1: Split 70% training dan 30% temporary data (validation + testing)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)

# Tahap 2: Split 30% temporary data menjadi 15% validation dan 15% testing
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Hasil pembagian
print(f"Training Data: {X_train.shape[0]} komentar")
print(f"Validation Data: {X_val.shape[0]} komentar")
print(f"Test Data: {X_test.shape[0]} komentar")

"""ada remove stopwords dan stemming

model svm
"""

# Vectorizing the text data
vectorizer = CountVectorizer()
X_train_vectorized = vectorizer.fit_transform(X_train)
X_test_vectorized = vectorizer.transform(X_test)

# Hyperparameter Tuning
param_grid = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto']
}

svm_model = SVC()
grid_search = GridSearchCV(
    estimator=svm_model,
    param_grid=param_grid,
    scoring='accuracy',
    cv=5,
    verbose=2,
    n_jobs=-1
)

grid_search.fit(X_train_vectorized, y_train)

print(f"Best Parameters: {grid_search.best_params_}")
print(f"Best Cross-Validation Score: {grid_search.best_score_:.2f}")

# Predict with best model
best_model = grid_search.best_estimator_
y_test_pred = best_model.predict(X_test_vectorized)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_test_pred)
print("\nConfusion Matrix:")
print(conf_matrix)

# Hitung metrik evaluasi
precision = precision_score(y_test, y_test_pred, average=None)
recall = recall_score(y_test, y_test_pred, average=None)
f1 = f1_score(y_test, y_test_pred, average=None)

# Akurasi keseluruhan
overall_accuracy = accuracy_score(y_test, y_test_pred)

# Cetak hasil evaluasi
classes = ["Bukan Ujaran Kebencian", "Ujaran Kebencian"]
print(f"\nAkurasi Keseluruhan: {overall_accuracy:.4f}\n")

print("Per-Class Metrics:")
for idx, label in enumerate(classes):
    print(f"Class: {label}")
    print(f"  Precision: {precision[idx]:.4f}")
    print(f"  Recall   : {recall[idx]:.4f}")
    print(f"  F1-Score : {f1[idx]:.4f}")
    print()

# Visualisasi Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=classes, yticklabels=classes)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Visualisasi Distribusi Prediksi
label_counts = np.unique(y_test_pred, return_counts=True)
labels, counts = label_counts

plt.figure(figsize=(6, 6))
plt.pie(counts, labels=labels, autopct='%1.1f%%',
        colors=['#66b3ff', '#99ff99', '#ffb3e6'])
plt.title('Distribution of Predicted Labels (SVM Model)')
plt.show()

"""model naive bayes

"""

# Membuat pipeline Naive Bayes
pipeline_nb = Pipeline([
    ('bow', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('classifier', MultinomialNB()),
])

# Melatih model
pipeline_nb.fit(X_train, y_train)

# Evaluasi pada data uji
y_pred = pipeline_nb.predict(X_test)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(conf_matrix)

# Hitung metrik evaluasi
precision = precision_score(y_test, y_pred, average=None)
recall = recall_score(y_test, y_pred, average=None)
f1 = f1_score(y_test, y_pred, average=None)

# Akurasi keseluruhan
overall_accuracy = accuracy_score(y_test, y_pred)

# Menampilkan metrik evaluasi
classes = ["Bukan Ujaran Kebencian", "Ujaran Kebencian"]
print("\n==== Evaluasi Model Naive Bayes ====")
print(f"\nAkurasi Keseluruhan: {overall_accuracy:.4f}\n")

for idx, label in enumerate(classes):
    print(f"Class: {label}")
    print(f"  Precision: {precision[idx]:.4f}")
    print(f"  Recall   : {recall[idx]:.4f}")
    print(f"  F1-Score : {f1[idx]:.4f}")
    print()

# Visualisasi Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=classes, yticklabels=classes)
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix - Naive Bayes')
plt.show()

# Cross-validation
cv_scores = cross_val_score(pipeline_nb, X, y, cv=5)
print(f"\nCross-validation scores (desimal): {cv_scores}")
print(f"Mean CV accuracy: {cv_scores.mean():.4f}")

"""tidak ada remove stopwords dan stemming"""

# Step 1: Load your dataset (replace 'your_dataset.csv' with the actual path to your CSV)
df = pd.read_csv('/content/Hasil prepocessing (5).csv')
df.head()

from sklearn.model_selection import train_test_split

# Split Data
X = df['databersih']
y = df['label']

# Tahap 1: Split 70% training dan 30% temporary data (validation + testing)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)

# Tahap 2: Split 30% temporary data menjadi 15% validation dan 15% testing
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Hasil pembagian
print(f"Training Data: {X_train.shape[0]} komentar")
print(f"Validation Data: {X_val.shape[0]} komentar")
print(f"Test Data: {X_test.shape[0]} komentar")

"""model svm"""

# Cek dan atasi NaN dalam X_train dan X_test
X_train = X_train.fillna("")
X_test = X_test.fillna("")

# Vectorizing the text data
vectorizer = CountVectorizer()
X_train_vectorized = vectorizer.fit_transform(X_train)
X_test_vectorized = vectorizer.transform(X_test)

# Hyperparameter Tuning with GridSearchCV
param_grid = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto']
}

svm_model = SVC()
grid_search = GridSearchCV(
    estimator=svm_model,
    param_grid=param_grid,
    scoring='accuracy',
    cv=5,
    verbose=2,
    n_jobs=-1
)

grid_search.fit(X_train_vectorized, y_train)

print(f"Best Parameters: {grid_search.best_params_}")
print(f"Best Cross-Validation Score: {grid_search.best_score_:.2f}")

# Evaluasi model terbaik
best_model = grid_search.best_estimator_
y_test_pred = best_model.predict(X_test_vectorized)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_test_pred)
print("\nConfusion Matrix:")
print(conf_matrix)

# Hitung metrik evaluasi
precision = precision_score(y_test, y_test_pred, average=None)
recall = recall_score(y_test, y_test_pred, average=None)
f1 = f1_score(y_test, y_test_pred, average=None)
overall_accuracy = accuracy_score(y_test, y_test_pred)

# Cetak hasil
classes = ["Bukan Ujaran Kebencian", "Ujaran Kebencian"]
print(f"\nAkurasi Keseluruhan: {overall_accuracy:.4f}\n")
print("Per-Class Metrics:")
for idx, label in enumerate(classes):
    print(f"Class: {label}")
    print(f"  Precision: {precision[idx]:.4f}")
    print(f"  Recall   : {recall[idx]:.4f}")
    print(f"  F1-Score : {f1[idx]:.4f}")
    print()

# Visualisasi Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=classes, yticklabels=classes)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Visualisasi distribusi label prediksi
label_counts = np.unique(y_test_pred, return_counts=True)
labels, counts = label_counts

plt.figure(figsize=(6, 6))
plt.pie(counts, labels=labels, autopct='%1.1f%%',
        colors=['#66b3ff', '#99ff99', '#ffb3e6'])
plt.title('Distribution of Predicted Labels (SVM Model)')
plt.show()

"""model naive bayes"""

# Pastikan tidak ada NaN
X_train = X_train.fillna('')
X_test = X_test.fillna('')

# Membuat pipeline
pipeline_nb = Pipeline([
    ('bow', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('classifier', MultinomialNB()),
])

# Latih model
pipeline_nb.fit(X_train, y_train)

# Prediksi
y_pred = pipeline_nb.predict(X_test)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(conf_matrix)

# Hitung metrik evaluasi
precision = precision_score(y_test, y_pred, average=None)
recall = recall_score(y_test, y_pred, average=None)
f1 = f1_score(y_test, y_pred, average=None)
overall_accuracy = accuracy_score(y_test, y_pred)

# Tampilkan hasil
print("\n==== Evaluasi Model Naive Bayes ====")
print(f"\nAkurasi Keseluruhan: {overall_accuracy:.4f}\n")
classes = ["Bukan Ujaran Kebencian", "Ujaran Kebencian"]
print("Per-Class Metrics:")
for idx, label in enumerate(classes):
    print(f"Class: {label}")
    print(f"  Precision: {precision[idx]:.4f}")
    print(f"  Recall   : {recall[idx]:.4f}")
    print(f"  F1-Score : {f1[idx]:.4f}")
    print()

# Visualisasi Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=classes, yticklabels=classes)
plt.title('Confusion Matrix - Naive Bayes')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Cross-validation
cv_scores = cross_val_score(pipeline_nb, X_train, y_train, cv=5)
cv_scores_decimal = [round(score, 4) for score in cv_scores]
print(f"Cross-validation scores (dalam desimal): {cv_scores_decimal}")
print(f"Mean CV accuracy: {cv_scores.mean():.4f}")

"""hybrid svm dan naive bayes"""

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import (
    confusion_matrix, accuracy_score,
    precision_score, recall_score, f1_score
)
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# 1. Load dataset
try:
    data = pd.read_csv('/content/Hasil prepocessing (5).csv')
    print("Dataset loaded successfully!")
except FileNotFoundError:
    print("Error: File not found.")
    exit()

# 2. Memisahkan fitur dan label
X = data['stemming'].astype(str)
y = data['label']

# 3. Split data
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# 4. Vectorization
vectorizer = CountVectorizer(max_features=5000)
X_train_vectorized = vectorizer.fit_transform(X_train)
X_val_vectorized = vectorizer.transform(X_val)
X_test_vectorized = vectorizer.transform(X_test)

# 5. Define models
nb_model = MultinomialNB()
svm_model = SVC(kernel='linear', probability=True, random_state=42)

# 6. Stacking Classifier
stacking_model = StackingClassifier(
    estimators=[('naive_bayes', nb_model), ('svm', svm_model)],
    final_estimator=LogisticRegression(),
    cv=5,
    n_jobs=-1
)

# 7. Grid Search
param_grid = {
    'final_estimator__C': [0.1, 1, 10]
}

grid_search = GridSearchCV(
    estimator=stacking_model,
    param_grid=param_grid,
    scoring='accuracy',
    cv=5,
    verbose=2,
    n_jobs=-1
)

# 8. Train model
grid_search.fit(X_train_vectorized, y_train)
print(f"Best Parameters: {grid_search.best_params_}")
print(f"Best Cross-Validation Score: {grid_search.best_score_:.4f}")

# 9. Evaluasi
best_model = grid_search.best_estimator_
y_test_pred = best_model.predict(X_test_vectorized)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_test_pred)
print("\nConfusion Matrix:")
print(conf_matrix)

# Hitung metrik evaluasi
precision = precision_score(y_test, y_test_pred, average=None)
recall = recall_score(y_test, y_test_pred, average=None)
f1 = f1_score(y_test, y_test_pred, average=None)
overall_accuracy = accuracy_score(y_test, y_test_pred)

# Tampilkan metrik
classes = ["Bukan Ujaran Kebencian", "Ujaran Kebencian"]
print(f"\nAkurasi Keseluruhan: {overall_accuracy:.4f}\n")
print("Per-Class Metrics:")
for idx, label in enumerate(classes):
    print(f"Class: {label}")
    print(f"  Precision: {precision[idx]:.4f}")
    print(f"  Recall   : {recall[idx]:.4f}")
    print(f"  F1-Score : {f1[idx]:.4f}")
    print()

# Visualisasi Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=classes)

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# 1. Load dataset
try:
    data = pd.read_csv('/content/Hasil prepocessing (5).csv')
    print("Dataset loaded successfully!")
except FileNotFoundError:
    print("Error: File not found. Please check the file path and try again.")
    exit()

# 2. Cek kolom
required_columns = ['databersih', 'label']
if not all(col in data.columns for col in required_columns):
    print("Error: Missing required columns in the dataset.")
    exit()

# 3. Pisahkan fitur dan label
X = data['databersih'].astype(str)
y = data['label']

# 4. Split data
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# 5. Vectorizer
vectorizer = CountVectorizer(max_features=5000)
X_train_vectorized = vectorizer.fit_transform(X_train)
X_val_vectorized = vectorizer.transform(X_val)
X_test_vectorized = vectorizer.transform(X_test)

# 6. Model
nb_model = MultinomialNB()
svm_model = SVC(kernel='linear', probability=True, random_state=42)

# 7. Stacking
stacking_model = StackingClassifier(
    estimators=[('naive_bayes', nb_model), ('svm', svm_model)],
    final_estimator=LogisticRegression(),
    cv=5,
    n_jobs=-1
)

# 8. Hyperparameter tuning
param_grid = {
    'final_estimator__C': [0.1, 1, 10]
}
grid_search = GridSearchCV(
    estimator=stacking_model,
    param_grid=param_grid,
    scoring='accuracy',
    cv=5,
    verbose=2,
    n_jobs=-1
)

# 9. Training
grid_search.fit(X_train_vectorized, y_train)

print(f"Best Parameters: {grid_search.best_params_}")
print(f"Best CV Accuracy: {grid_search.best_score_:.4f}")

# 10. Evaluasi
best_model = grid_search.best_estimator_
y_test_pred = best_model.predict(X_test_vectorized)

# Confusion matrix
conf_matrix = confusion_matrix(y_test, y_test_pred)
print("\nConfusion Matrix:")
print(conf_matrix)

# Metrics
precision = precision_score(y_test, y_test_pred, average=None)
recall = recall_score(y_test, y_test_pred, average=None)
f1 = f1_score(y_test, y_test_pred, average=None)
overall_accuracy = accuracy_score(y_test, y_test_pred)

# Label kelas (urutannya harus sama dengan confusion_matrix)
classes = ["Bukan Ujaran Kebencian", "Ujaran Kebencian"]

print(f"\nAkurasi Keseluruhan: {overall_accuracy:.4f}\n")

# Print metrik untuk tiap kelas
for i, class_name in enumerate(classes):
    print(f"Class: {class_name}")
    print(f"  Precision: {precision[i]:.4f}")
    print(f"  Recall   : {recall[i]:.4f}")
    print(f"  F1-Score : {f1[i]:.4f}")
    print()

# Visualisasi Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=classes, yticklabels=classes)
plt.title('Confusion Matrix - Stacking Model')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Visualisasi Distribusi Prediksi
labels, counts = np.unique(y_test_pred, return_counts=True)
plt.figure(figsize=(6, 6))
plt.pie(counts, labels=labels, autopct='%1.1f%%',
        colors=['#66b3ff', '#99ff99', '#ffb3e6'])
plt.title('Distribution of Predicted Labels (Stacking Model)')
plt.show()